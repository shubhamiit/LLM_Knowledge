{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune T5 for summarization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #install libs\n",
    "\n",
    "# %pip install --upgrade pip\n",
    "# %pip install --disable-pip-version-check \\\n",
    "#     torch==1.13.1 \\\n",
    "#     torchdata==0.5.1 --quiet\n",
    "\n",
    "# %pip install \\\n",
    "#     transformers==4.27.2 \\\n",
    "#     datasets==2.11.0 \\\n",
    "#     evaluate==0.4.0 \\\n",
    "#     rouge_score==0.1.2 \\\n",
    "#     loralib==0.1.1 \\\n",
    "#     peft==0.3.0 --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/LLM3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libs\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TIME=str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshubsoni\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/sonishubhamnc6/code/Users/sonishubham/wandb/run-20230928_170240-i78i2o2l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shubsoni/T5-peft-finetuning/runs/i78i2o2l' target=\"_blank\">hopeful-dust-4</a></strong> to <a href='https://wandb.ai/shubsoni/T5-peft-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shubsoni/T5-peft-finetuning' target=\"_blank\">https://wandb.ai/shubsoni/T5-peft-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shubsoni/T5-peft-finetuning/runs/i78i2o2l' target=\"_blank\">https://wandb.ai/shubsoni/T5-peft-finetuning/runs/i78i2o2l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/shubsoni/T5-peft-finetuning/runs/i78i2o2l?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f42af880340>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add wandb logging\n",
    "# %pip install wandb\n",
    "import wandb\n",
    "wandb.init(project=\"T5-peft-finetuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/azureuser/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 745.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hugging_face_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(Hugging_face_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pretrained FLAN-T5 model and tokenizer\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype = torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def print_number_of_trainable_parameters(model):\n",
    "    trainable_params =0\n",
    "    all_model_params =0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params+=param.numel()\n",
    "    return f\"trainable model parameters: {trainable_params}\\nall model parameters: {all_model_params} \\n. percentage trainable parameters: {trainable_params*(100)/all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainable model parameters: 247577856\\nall model parameters: 247577856 \\n. percentage trainable parameters: 100.00%'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_number_of_trainable_parameters(original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Model for zero shot inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INTIAL PROMPT:\n",
      ":\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMMAN SUMMARY:\n",
      " #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION- ZERO SHOT:\n",
      " #Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index =200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(original_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=200,\n",
    ")[0],\n",
    "skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f\"INTIAL PROMPT:\\n:{prompt}\")\n",
    "print(dash_line)\n",
    "print(f\"BASELINE HUMMAN SUMMARY:\\n {summary}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"MODEL GENERATION- ZERO SHOT:\\n {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the dataset to add instruction prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/azureuser/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-17819e0af33e0661.arrow\n",
      "Loading cached processed dataset at /home/azureuser/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-42fdf05daa718908.arrow\n",
      "Loading cached processed dataset at /home/azureuser/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-3d8db1d9c7b40260.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    system_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    trigger_prompt = '\\n\\nSummary: '\n",
    "    prompt = [system_prompt + dialogue + trigger_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\",truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenize_dataset = tokenized_dataset.remove_columns(['id', 'topic', 'dialogue', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Summarize the following conversation. #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today? #Person2#: I found it would be a good idea to get a check-up. #Person1#: Yes, well, you haven't had one for 5 years. You should have one every year. #Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor? #Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good. #Person2#: Ok. #Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith? #Person2#: Yes. #Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit. #Person2#: I've tried hundreds of times, but I just can't seem to kick the habit. #Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave. #Person2#: Ok, thanks doctor. Summary: \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenize_dataset['train'][0]['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Todo: Rmove, take a sample\n",
    "# #\n",
    "# tokenized_dataset = tokenized_dataset.filter(lambda example, index: index % 5==0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the datasets\n",
      "\n",
      "Training: (12460, 6)\n",
      "Validation: (500, 6)\n",
      "Test: (1500, 6)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# check the size of the datasets\n",
    "\n",
    "print(\"shape of the datasets\\n\")\n",
    "print(f\"Training: {tokenized_dataset['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_dataset['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_dataset['test'].shape}\")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the PEFT adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=128,\n",
    "    # target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM #T5\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainable model parameters: 884736\\nall model parameters: 248462592 \\n. percentage trainable parameters: 0.36%'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add lora layers to the original LLM\n",
    "\n",
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "\n",
    "print_number_of_trainable_parameters(peft_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    # we can use higher learning rate for peft adapter in comparison to full finetuning\n",
    "    learning_rate = 1e-03,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    # max_steps =100\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model = peft_model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    eval_dataset= tokenized_dataset['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/LLM3.10/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='209' max='1558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 209/1558 03:12 < 20:57, 1.07 it/s, Epoch 0.13/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.728900</td>\n",
       "      <td>0.111352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.101086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "peft_model_path = f\"./peft-dialogue-summary-checkpoint-{TIME}\"\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainable model parameters: 0\\nall model parameters: 248462592 \\n. percentage trainable parameters: 0.00%'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the adapter to the original T-5 Model and set trainable as False, as we will only use it for inference\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "# we can switch different adapter with base model\n",
    "instruct_model = PeftModel.from_pretrained(original_model,\n",
    "                              peft_model_path,\n",
    "                              torch_dtype= torch.bfloat16,\n",
    "                              is_trainable=False)\n",
    "\n",
    "\n",
    "print_number_of_trainable_parameters(instruct_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model in GPU\n",
    "\n",
    "instruct_model= instruct_model.to(\"cuda\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype = torch.bfloat16)\n",
    "base_model = base_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model Quality [Humman Evaluation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Base line humman summary:\n",
      " #Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "base_model: \n",
      " The two of them will try to figure out how to express their feelings.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "instruct_model:\n",
      " #Person1# tells Mike that she doesn't want to see him anymore, but he wants to get more anger from Mike.\n"
     ]
    }
   ],
   "source": [
    "index =100\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, max_new_tokens =200)\n",
    "instruct_model_decoded_outputs = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "base_model_ouputs = base_model.generate(input_ids=input_ids, max_new_tokens =200)\n",
    "base_model_decoded_output = tokenizer.decode(base_model_ouputs[0],skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "print(dash_line)\n",
    "print(f\"Base line humman summary:\\n {human_baseline_summary}\")\n",
    "print(dash_line)\n",
    "print(f\"base_model: \\n {base_model_decoded_output}\")\n",
    "print(dash_line)\n",
    "print(f\"instruct_model:\\n {instruct_model_decoded_outputs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model quantitatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>base_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>#Person1# wants to take a dictation for Ms. Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>#Person1# wants to take a dictation for Ms. Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>#Person1# wants to take a dictation for Ms. Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2# thinks it's better for the environme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2# thinks it's better for the environme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "\n",
       "                                base_model_summaries  \\\n",
       "0     #Person1#: I need to take a dictation for you.   \n",
       "1     #Person1#: I need to take a dictation for you.   \n",
       "2     #Person1#: I need to take a dictation for you.   \n",
       "3  The traffic jam at the Carrefour intersection ...   \n",
       "4  The traffic jam at the Carrefour intersection ...   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0  #Person1# wants to take a dictation for Ms. Da...  \n",
       "1  #Person1# wants to take a dictation for Ms. Da...  \n",
       "2  #Person1# wants to take a dictation for Ms. Da...  \n",
       "3  #Person2# thinks it's better for the environme...  \n",
       "4  #Person2# thinks it's better for the environme...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "dialogues = dataset['test'][0:5]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:5]['summary']\n",
    "\n",
    "base_model_summaries =[]\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config = GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "    instruct_model_decoded_outputs = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_decoded_outputs)\n",
    "\n",
    "    base_model_ouputs = base_model.generate(input_ids=input_ids, generation_config= GenerationConfig(max_new_tokens=200,num_beams =1))\n",
    "    base_model_decoded_output = tokenizer.decode(base_model_ouputs[0],skip_special_tokens=True)\n",
    "    base_model_summaries.append(base_model_decoded_output)\n",
    "\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, base_model_summaries, instruct_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'base_model_summaries', 'instruct_model_summaries'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Base line humman summary:\n",
      " ['Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.', 'In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.', 'Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.', '#Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.', \"#Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.\"]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "base_model: \n",
      " ['#Person1#: I need to take a dictation for you.', '#Person1#: I need to take a dictation for you.', '#Person1#: I need to take a dictation for you.', 'The traffic jam at the Carrefour intersection is a problem.', 'The traffic jam at the Carrefour intersection is a problem.']\n",
      "---------------------------------------------------------------------------------------------------\n",
      "instruct_model:\n",
      " ['#Person1# wants to take a dictation for Ms. Dawson. #Person2# wants to take a dictation for her.', '#Person1# wants to take a dictation for Ms. Dawson. #Person2# wants to take a dictation for her.', '#Person1# wants to take a dictation for Ms. Dawson. #Person2# wants to take a dictation for her.', \"#Person2# thinks it's better for the environment and the environment. #Person2# thinks it's good for the environment.\", \"#Person2# thinks it's better for the environment and the environment. #Person2# thinks it's good for the environment.\"]\n"
     ]
    }
   ],
   "source": [
    "print(dash_line)\n",
    "print(f\"Base line humman summary:\\n {human_baseline_summaries}\")\n",
    "print(dash_line)\n",
    "print(f\"base_model: \\n {base_model_summaries}\")\n",
    "print(dash_line)\n",
    "print(f\"instruct_model:\\n {instruct_model_summaries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.15305555555555556, 'rouge2': 0.04862745098039215, 'rougeL': 0.14194444444444443, 'rougeLsum': 0.1422222222222222}\n",
      "INSTRUCT MODEL\n",
      "{'rouge1': 0.2438242722230418, 'rouge2': 0.06498599439775911, 'rougeL': 0.20532150776053215, 'rougeLsum': 0.20679544738933892}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the rouge on both model summaries to compare them side by side\n",
    "\n",
    "base_model_results = rouge.compute(\n",
    "    predictions = base_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer = True\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions = instruct_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer = True\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(base_model_results)\n",
    "print('INSTRUCT MODEL')\n",
    "print(instruct_model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of INSTRUCT MODEL over Humman Baseline\n",
      "rouge1: 9.08%\n",
      "rouge2: 1.64%\n",
      "rougeL: 6.34%\n",
      "rougeLsum: 6.46%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over Humman Baseline\")\n",
    "\n",
    "improvement = (np.array(list(instruct_model_results.values()))- np.array(list(base_model_results.values())))\n",
    "\n",
    "for key, value in zip(instruct_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
